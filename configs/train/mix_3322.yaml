# usa questo AL POSTO di baseline.yaml per allenare su 4 task insieme
tokenizer_file: "data/vocab/bpe.json"

datasets:
  - name: "rdf2text"
    train: "data/processed/rdf2text.train.jsonl"
    val:   "data/processed/rdf2text.val.jsonl"
    weight: 3
  - name: "text2rdf"
    train: "data/processed/text2rdf.train.jsonl"
    val:   "data/processed/text2rdf.val.jsonl"
    weight: 3
  - name: "rdfcomp1"
    train: "data/processed/rdfcomp1.train.jsonl"
    val:   "data/processed/rdfcomp1.val.jsonl"
    weight: 2
  - name: "rdfcomp2"
    train: "data/processed/rdfcomp2.train.jsonl"
    val:   "data/processed/rdfcomp2.val.jsonl"
    weight: 2

# modello
use_rope: false
use_mla: false
interleave_ratio: 0.0
d_model: 384
nhead: 6
enc_layers: 3
dec_layers: 3
ff_dim: 1536
dropout: 0.1
max_len: 512

# span masking per RDF Completion 1
enable_entity_spans: true
compute_span_metrics: true

# training
batch_size: 16
num_epochs: 10
gradient_accumulation_steps: 1
lr: 3e-4
weight_decay: 0.01
scheduler: "cosine"
warmup_ratio: 0.05
min_lr_ratio: 0.02
save_dir: "checkpoints/mix3322"
seed: 42
device: "cuda"
num_workers: 4

wandb:
  mode: "disabled"
  project: "nanosocrates"
  entity: null
  run_name: null
  tags: ["multitask"]
  watch: false
