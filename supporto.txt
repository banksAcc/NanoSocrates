TEST DB - ENDPOINT E DATI

# 1) Triple DBpedia
python scripts/fetch_dbpedia.py --config configs/data/dbpedia.yaml --out data/raw/dbpedia_triples.jsonl

# 2) Testo Wikipedia
python scripts/fetch_wikipedia.py --config configs/data/wikipedia.yaml --in data/raw/dbpedia_triples.jsonl --out data/raw/wikipedia_intro.jsonl --max 200 --workers 8 --cache data/interim/wiki_cache.jsonl --logfails data/interim/wiki_failures.jsonl

# 3) Costruzione dataset (split per film + 4 task per split)
python scripts/build_dataset.py --config configs/data/build.yaml --dbp data/raw/dbpedia_triples.jsonl --wiki data/raw/wikipedia_intro.jsonl --outdir data/processed


python scripts/train_tokenizer.py --config configs/tokenizer/bpe_24k.yaml \
  --inputs data/processed/text2rdf.train.jsonl data/processed/rdf2text.train.jsonl \
  --outdir data/vocab

 TEST tokenizer 

 1) Tokenizer BPE (usa i file emessi da --emit_tasks)
 python scripts/train_tokenizer.py --config configs/tokenizer/bpe_24k.yaml
 python -m scripts.train_tokenizer --config configs/tokenizer/bpe_24k.yaml

pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio


2) Sanity overfit (sovrascrivi qualche parametro al volo)
python -m src.cli overfit --cfg configs/train/baseline.yaml --override max_steps=500 batch_size=8 train_file=data/processed/text2rdf.train.jsonl
python -m src.cli overfit --cfg configs/train/baseline.yaml --override max_steps=500 batch_size=8

python -m src.cli overfit --cfg configs/train/baseline.yaml --override max_steps=300 batch_size=4 train_file=data/processed/_mini.train.jsonl val_file=data/processed/_mini.train.jsonl

3) Training baseline (addestramento per un solo task)
python -m src.cli train --cfg configs/train/baseline.yaml
4) Multi-task 3:3:2:2 (tutti e 4 i task):
python -m src.cli train --cfg configs/train/mix_3322.yaml

